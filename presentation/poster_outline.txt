ISIT 2025 Competition Poster (Detailed Content)
===============================================

Title: Semi-Supervised Indoor Positioning with DICHASUS Massive-MIMO CSI

1. Introduction
----------------
- **Competition:** ISIT 2025, University of Stuttgart
- **Dataset:** DICHASUS Massive-MIMO CSI
  * 4 × 8 antenna elements
  * 16 subcarriers
  * Complex-valued measurements (2 channels)
  * Multiple time steps for trajectory
- **Objective:** Accurate indoor localization under communication constraints
- **Significance:** Enables smart environments, IoT, and robotics
- *Visual: Competition logo, dataset schematic, indoor positioning diagram*
  - **Description:** The competition logo and dataset schematic introduce the context. The indoor positioning diagram illustrates the challenge of localizing users in a complex environment using wireless signals.

2. Approach
------------
- **Data Pipeline:**
  * Raw CSI → Preprocessing → Feature extraction → Model input
  * Preprocessing:
    - Min-max scaling per subcarrier
    - IQR-based outlier removal
    - Zero-padding with mask for missing data
  * Feature engineering:
    - Statistical: Mean, std, kurtosis per array
    - Temporal: 10-frame sliding window
    - Spatial: Array correlation matrices
  * *Visual: Flowchart of data pipeline with sample transformations*
  - **Description:** This flowchart shows the step-by-step transformation from raw CSI to model-ready features, highlighting preprocessing and feature engineering steps crucial for robust model performance.

- **Task-Specific Models:**
  * Task 1 (Vanilla):
    - 3D-CNN backbone: Conv3D(32,64,128) + BatchNorm
    - Dense layers: 256→128→2 with 0.3 dropout
    - Performance: MAE = 0.42m, R90 = 0.85m
  
  * Task 2 (Trajectory):
    - CNN-LSTM hybrid with attention
    - LSTM(128) + 4-head attention
    - Performance: MAE = 0.35m, R90 = 0.72m
    - 17% improvement over Task 1
  
  * Task 3 (Grant-Free):
    - Local-Central architecture
    - Policy network for transmission decisions
    - Performance: MAE = 0.38m, 40% transmission rate
  
  * Task 4 (Joint):
    - Hybrid model with compression
    - Autoencoder (32→512→1024)
    - Performance: MAE = 0.36m, 35% transmission rate
  * *Visual: Block diagrams of model architectures with performance metrics*
  - **Description:** Each block diagram details the architecture for a specific task, showing the flow of data through layers (e.g., 3D-CNN, LSTM, attention, autoencoder). Performance metrics (MAE, R90, transmission rate) are annotated to demonstrate the effectiveness of each design.

- **Training:**
  * Semi-supervised learning:
    - 20% labeled data
    - Consistency regularization
    - Data augmentation (noise, rotations)
  * Optimization:
    - Adam (lr=0.001, β1=0.9, β2=0.999)
    - Batch size: 32
    - Early stopping (10 epochs)
  * *Visual: Training workflow with loss curves*
  - **Description:** The training workflow diagram is accompanied by loss curves (from the generated plots), illustrating convergence and the impact of semi-supervised learning and regularization.

3. Results
-----------
- **Performance Metrics:**
  * Task 1 (Vanilla):
    - MAE = 0.42m, R90 = 0.85m
    - Combined = 0.56m
    - Inference: 5ms, Model: 12MB
  
  * Task 2 (Trajectory):
    - MAE = 0.35m, R90 = 0.72m
    - Combined = 0.47m
    - Inference: 8ms
  
  * Task 3 (Grant-Free):
    - MAE = 0.38m, R90 = 0.78m
    - Combined = 0.51m
    - 60% bandwidth reduction
  
  * Task 4 (Joint):
    - MAE = 0.36m, R90 = 0.74m
    - Combined = 0.49m
    - 65% bandwidth reduction
  * *Visual: Comparative bar charts and radar plots*
  - **Description:** Bar charts compare MAE, R90, and combined metrics across tasks, making it easy to see which models perform best. Radar plots visualize trade-offs between accuracy, transmission rate, and model complexity, helping the reader grasp the strengths of each approach.

- **Visualizations:**
  * Training curves:
    - Loss evolution
    - Metric progression
  * Error analysis:
    - Position scatter plots
    - Error heatmaps
    - Trajectory comparisons
  * *Visual: Selected plots from results*
  - **Description:** Training curves show how loss and metrics evolve, while error analysis plots (scatter, heatmap, trajectory) reveal where models succeed or struggle. These visuals provide evidence for claims about model performance and robustness.

4. Key Insights
----------------
- **Technical Findings:**
  * Trajectory information improves accuracy by 17%
  * Attention mechanism crucial for temporal modeling
  * 3D-CNN effective for spatial CSI processing
  * Compression ratio of 32x maintains accuracy
  * Joint optimization achieves better trade-off
  * *Visual: Key findings as icons with metrics*
  - **Description:** Icons and annotated metrics summarize the most important technical findings, making insights accessible at a glance.

- **Challenges & Solutions:**
  * Data quality: Robust preprocessing pipeline
  * Model complexity: Efficient architectures
  * Real-time constraints: Optimized inference
  * *Visual: Key findings as icons with metrics*

5. Conclusion & Future Work
---------------------------
- **Current Achievements:**
  * Hierarchical model design
  * Semi-supervised learning
  * Joint optimization strategies
  * Efficient inference (< 10ms)
  * *Visual: Roadmap or future directions graphic*
  - **Description:** The roadmap graphic visually outlines planned technical and practical improvements, guiding the reader through future research directions.

- **Future Directions:**
  * Technical:
    - Graph neural networks
    - Transformer architectures
    - Advanced compression
  * Practical:
    - Real-world deployment
    - Transfer learning
    - Edge optimization
  * *Visual: Roadmap or future directions graphic*

6. Contact & Acknowledgements
-----------------------------
- Team members and affiliations
- Contact information
- GitHub repository
- Project website
- *Visual: QR code to project page*
- **Description:** The QR code links directly to the project website, enabling quick access to more information and resources.
- Acknowledgements:
  * Competition organizers
  * Dataset providers
  * Collaborators
  * Open-source community 